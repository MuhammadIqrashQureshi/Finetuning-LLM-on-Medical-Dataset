# Fast Training Configuration for RTX 3060 6GB
# Optimized for SPEED while maintaining good quality

# Model Configuration
model:
  base_model: "unsloth/llama-3-8b-Instruct-bnb-4bit"
  max_seq_length: 1024  # REDUCED from 2048 - 40% faster
  load_in_4bit: true

# Dataset
data:
  dataset_path: "data/merged/medical_merged_15k.jsonl"
  text_column: "prompt"
  
# LoRA Configuration - Reduced for speed
lora:
  r: 8                   # REDUCED from 16 - faster
  lora_alpha: 16
  lora_dropout: 0.05
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj

# Training Arguments - Optimized for RTX 3060 6GB
training:
  output_dir: "./outputs/medical_llama3_fast"
  num_train_epochs: 1    # 1 epoch
  per_device_train_batch_size: 2  # Increased slightly 
  gradient_accumulation_steps: 4  # Effective batch = 8
  learning_rate: 0.0002
  weight_decay: 0.01
  warmup_ratio: 0.03
  lr_scheduler_type: "cosine"
  logging_steps: 50
  save_strategy: "steps"
  save_steps: 500
  fp16: true
  gradient_checkpointing: true
  optim: "adamw_8bit"
  max_grad_norm: 0.3
  dataloader_num_workers: 0
  
# Ollama Export
export:
  quantization: "q4_k_m"
  model_name: "medical-llama3-fast"
