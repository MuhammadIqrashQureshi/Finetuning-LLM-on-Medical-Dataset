{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edd39bc3",
   "metadata": {},
   "source": [
    "# üè• Medical LLM Fine-Tuning with LoRA/QLoRA\n",
    "\n",
    "This notebook demonstrates how to fine-tune a Large Language Model on medical data using Parameter Efficient Fine-Tuning (PEFT) techniques.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We will:\n",
    "1. Install and import required libraries\n",
    "2. Load a base LLM model with 4-bit quantization\n",
    "3. Prepare medical datasets for instruction tuning\n",
    "4. Configure LoRA/QLoRA parameters\n",
    "5. Fine-tune the model\n",
    "6. Export to GGUF format for Ollama\n",
    "7. Test the fine-tuned model\n",
    "\n",
    "**Based on:** [LLM-Medical-Finetuning](https://github.com/Shekswess/LLM-Medical-Finetuning)\n",
    "\n",
    "‚ö†Ô∏è **Disclaimer:** This model is for educational purposes only. Always consult healthcare professionals for medical advice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd502e9",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries\n",
    "\n",
    "First, let's install all necessary packages. Choose the appropriate installation based on your GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fd5c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages for RTX 30xx, 40xx, A100, H100, L40 GPUs\n",
    "# Uncomment and run if needed\n",
    "# !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
    "# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "# For older GPUs (V100, T4, RTX 20xx) - no flash attention\n",
    "# !pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
    "# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "# Standard installation without unsloth (works on most systems)\n",
    "# !pip install torch transformers datasets peft trl accelerate bitsandbytes huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0844f227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Try to import unsloth (for faster training)\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    UNSLOTH_AVAILABLE = True\n",
    "    print(\"‚úÖ Unsloth is available - using optimized training!\")\n",
    "except ImportError:\n",
    "    UNSLOTH_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è Unsloth not available - using standard transformers\")\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "    from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"\\nüñ•Ô∏è CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üìä GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"üî¢ BF16 Support: {torch.cuda.is_bf16_supported()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9348691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Hugging Face Hub (required for some models and pushing to hub)\n",
    "# You'll need a HuggingFace account and access token\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429f80c8",
   "metadata": {},
   "source": [
    "## 2. Load Base Model with 4-bit Quantization\n",
    "\n",
    "We'll use a 4-bit quantized model to reduce memory requirements. Available models:\n",
    "- `unsloth/llama-3-8b-Instruct-bnb-4bit` - Llama 3 (8B)\n",
    "- `unsloth/llama-2-7b-chat-bnb-4bit` - Llama 2 (7B)\n",
    "- `unsloth/mistral-7b-instruct-v0.2-bnb-4bit` - Mistral (7B)\n",
    "- `unsloth/gemma-1.1-7b-it-bnb-4bit` - Gemma (7B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca77ddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for the fine-tuning\n",
    "config = {\n",
    "    \"hugging_face_username\": \"your_username\",  # Replace with your HF username\n",
    "    \n",
    "    \"model_config\": {\n",
    "        \"base_model\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",  # Base model to fine-tune\n",
    "        \"finetuned_model\": \"llama-3-8b-medical\",  # Name for your fine-tuned model\n",
    "        \"max_seq_length\": 2048,  # Maximum sequence length\n",
    "        \"dtype\": torch.float16,  # Data type\n",
    "        \"load_in_4bit\": True,  # Use 4-bit quantization\n",
    "    },\n",
    "    \n",
    "    \"lora_config\": {\n",
    "        \"r\": 16,  # LoRA rank (8, 16, 32, 64)\n",
    "        \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                          \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        \"lora_alpha\": 16,\n",
    "        \"lora_dropout\": 0,\n",
    "        \"bias\": \"none\",\n",
    "        \"use_gradient_checkpointing\": True,\n",
    "        \"use_rslora\": False,\n",
    "        \"use_dora\": False,\n",
    "        \"loftq_config\": None\n",
    "    },\n",
    "    \n",
    "    \"training_dataset\": {\n",
    "        \"name\": \"Shekswess/medical_llama3_instruct_dataset_short\",  # Short dataset (2000 samples)\n",
    "        # \"name\": \"Shekswess/medical_llama3_instruct_dataset\",  # Full dataset\n",
    "        \"split\": \"train\",\n",
    "        \"input_field\": \"prompt\",\n",
    "    },\n",
    "    \n",
    "    \"training_config\": {\n",
    "        \"per_device_train_batch_size\": 2,\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "        \"warmup_steps\": 5,\n",
    "        \"max_steps\": 0,  # 0 to use num_train_epochs instead\n",
    "        \"num_train_epochs\": 1,\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"fp16\": not torch.cuda.is_bf16_supported() if torch.cuda.is_available() else True,\n",
    "        \"bf16\": torch.cuda.is_bf16_supported() if torch.cuda.is_available() else False,\n",
    "        \"logging_steps\": 1,\n",
    "        \"optim\": \"adamw_8bit\",\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"lr_scheduler_type\": \"linear\",\n",
    "        \"seed\": 42,\n",
    "        \"output_dir\": \"outputs\",\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìã Configuration loaded!\")\n",
    "print(f\"   Base model: {config['model_config']['base_model']}\")\n",
    "print(f\"   LoRA rank: {config['lora_config']['r']}\")\n",
    "print(f\"   Dataset: {config['training_dataset']['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e321bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "if UNSLOTH_AVAILABLE:\n",
    "    # Using Unsloth for optimized loading\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=config[\"model_config\"][\"base_model\"],\n",
    "        max_seq_length=config[\"model_config\"][\"max_seq_length\"],\n",
    "        dtype=config[\"model_config\"][\"dtype\"],\n",
    "        load_in_4bit=config[\"model_config\"][\"load_in_4bit\"],\n",
    "    )\n",
    "    print(\"‚úÖ Model loaded with Unsloth!\")\n",
    "else:\n",
    "    # Standard transformers loading with 4-bit quantization\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        config[\"model_config\"][\"base_model\"].replace(\"unsloth/\", \"\"),\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config[\"model_config\"][\"base_model\"].replace(\"unsloth/\", \"\"),\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"‚úÖ Model loaded with standard transformers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e7d148",
   "metadata": {},
   "source": [
    "## 3. Prepare Medical Dataset\n",
    "\n",
    "Load the pre-processed medical instruction dataset from HuggingFace. The dataset contains medical Q&A pairs formatted for instruction tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec60b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the medical dataset\n",
    "dataset_train = load_dataset(\n",
    "    config[\"training_dataset\"][\"name\"], \n",
    "    split=config[\"training_dataset\"][\"split\"]\n",
    ")\n",
    "\n",
    "print(f\"üìä Dataset Statistics:\")\n",
    "print(f\"   Number of samples: {len(dataset_train)}\")\n",
    "print(f\"   Columns: {dataset_train.column_names}\")\n",
    "print(f\"\\nüìù Sample prompt preview:\")\n",
    "print(\"-\" * 50)\n",
    "print(dataset_train[0][\"prompt\"][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d88382",
   "metadata": {},
   "source": [
    "## 4. Configure LoRA/QLoRA Parameters\n",
    "\n",
    "Set up the PEFT (Parameter Efficient Fine-Tuning) configuration with LoRA adapters. This allows us to fine-tune only a small number of parameters while keeping the base model frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67737bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup LoRA/QLoRA adapters\n",
    "if UNSLOTH_AVAILABLE:\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=config[\"lora_config\"][\"r\"],\n",
    "        target_modules=config[\"lora_config\"][\"target_modules\"],\n",
    "        lora_alpha=config[\"lora_config\"][\"lora_alpha\"],\n",
    "        lora_dropout=config[\"lora_config\"][\"lora_dropout\"],\n",
    "        bias=config[\"lora_config\"][\"bias\"],\n",
    "        use_gradient_checkpointing=config[\"lora_config\"][\"use_gradient_checkpointing\"],\n",
    "        random_state=42,\n",
    "        use_rslora=config[\"lora_config\"][\"use_rslora\"],\n",
    "        use_dora=config[\"lora_config\"][\"use_dora\"],\n",
    "        loftq_config=config[\"lora_config\"][\"loftq_config\"],\n",
    "    )\n",
    "else:\n",
    "    # Standard PEFT setup\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    peft_config = LoraConfig(\n",
    "        r=config[\"lora_config\"][\"r\"],\n",
    "        lora_alpha=config[\"lora_config\"][\"lora_alpha\"],\n",
    "        lora_dropout=config[\"lora_config\"][\"lora_dropout\"],\n",
    "        bias=config[\"lora_config\"][\"bias\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=config[\"lora_config\"][\"target_modules\"],\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"üîß Trainable parameters: {trainable_params:,} / {all_param:,} ({100 * trainable_params / all_param:.2f}%)\")\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1337093e",
   "metadata": {},
   "source": [
    "## 5. Set Up Training Arguments\n",
    "\n",
    "Configure the training hyperparameters including learning rate, batch size, and optimization settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc39e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "os.makedirs(config[\"training_config\"][\"output_dir\"], exist_ok=True)\n",
    "\n",
    "# Setup the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset_train,\n",
    "    dataset_text_field=config[\"training_dataset\"][\"input_field\"],\n",
    "    max_seq_length=config[\"model_config\"][\"max_seq_length\"],\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,  # Can set to True for short sequences\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=config[\"training_config\"][\"per_device_train_batch_size\"],\n",
    "        gradient_accumulation_steps=config[\"training_config\"][\"gradient_accumulation_steps\"],\n",
    "        warmup_steps=config[\"training_config\"][\"warmup_steps\"],\n",
    "        max_steps=config[\"training_config\"][\"max_steps\"] if config[\"training_config\"][\"max_steps\"] > 0 else -1,\n",
    "        num_train_epochs=config[\"training_config\"][\"num_train_epochs\"],\n",
    "        learning_rate=config[\"training_config\"][\"learning_rate\"],\n",
    "        fp16=config[\"training_config\"][\"fp16\"],\n",
    "        bf16=config[\"training_config\"][\"bf16\"],\n",
    "        logging_steps=config[\"training_config\"][\"logging_steps\"],\n",
    "        optim=config[\"training_config\"][\"optim\"],\n",
    "        weight_decay=config[\"training_config\"][\"weight_decay\"],\n",
    "        lr_scheduler_type=config[\"training_config\"][\"lr_scheduler_type\"],\n",
    "        seed=config[\"training_config\"][\"seed\"],\n",
    "        output_dir=config[\"training_config\"][\"output_dir\"],\n",
    "        report_to=\"none\",  # Set to \"wandb\" for Weights & Biases logging\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer configured!\")\n",
    "print(f\"   Batch size: {config['training_config']['per_device_train_batch_size']}\")\n",
    "print(f\"   Gradient accumulation: {config['training_config']['gradient_accumulation_steps']}\")\n",
    "print(f\"   Effective batch size: {config['training_config']['per_device_train_batch_size'] * config['training_config']['gradient_accumulation_steps']}\")\n",
    "print(f\"   Learning rate: {config['training_config']['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175b70d6",
   "metadata": {},
   "source": [
    "## 6. Fine-Tune the Model\n",
    "\n",
    "Now we'll run the training loop. This may take a while depending on your GPU and dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c664964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory statistics before training\n",
    "if torch.cuda.is_available():\n",
    "    gpu_stats = torch.cuda.get_device_properties(0)\n",
    "    reserved_memory = round(torch.cuda.max_memory_reserved() / 1024**3, 2)\n",
    "    max_memory = round(gpu_stats.total_memory / 1024**3, 2)\n",
    "    print(f\"üíæ Memory before training:\")\n",
    "    print(f\"   Reserved: {reserved_memory} GB\")\n",
    "    print(f\"   Total: {max_memory} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7342e092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Start training!\n",
    "print(\"üöÄ Starting training...\")\n",
    "trainer_stats = trainer.train()\n",
    "print(\"‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23910582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory statistics after training\n",
    "if torch.cuda.is_available():\n",
    "    used_memory = round(torch.cuda.max_memory_allocated() / 1024**3, 2)\n",
    "    used_memory_lora = round(used_memory - reserved_memory, 2)\n",
    "    used_memory_pct = round((used_memory / max_memory) * 100, 2)\n",
    "    \n",
    "    print(f\"\\nüíæ Memory after training:\")\n",
    "    print(f\"   Used: {used_memory} GB ({used_memory_pct}%)\")\n",
    "    print(f\"   Used for LoRA training: {used_memory_lora} GB\")\n",
    "    \n",
    "# Save training stats\n",
    "with open(os.path.join(config[\"training_config\"][\"output_dir\"], \"trainer_stats.json\"), \"w\") as f:\n",
    "    json.dump({\n",
    "        \"training_loss\": trainer_stats.training_loss,\n",
    "        \"global_step\": trainer_stats.global_step,\n",
    "    }, f, indent=4)\n",
    "print(\"üìä Training stats saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dca41db",
   "metadata": {},
   "source": [
    "## 7. Save and Export Fine-Tuned Model\n",
    "\n",
    "Save the model locally and optionally export to GGUF format for use with Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2a5418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model locally\n",
    "model_save_path = os.path.join(config[\"training_config\"][\"output_dir\"], config[\"model_config\"][\"finetuned_model\"])\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "print(f\"‚úÖ Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0508c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to GGUF format for Ollama (requires unsloth)\n",
    "# Choose quantization: \"q4_k_m\" (balanced), \"q8_0\" (higher quality), \"f16\" (full precision)\n",
    "\n",
    "if UNSLOTH_AVAILABLE:\n",
    "    gguf_output_path = os.path.join(config[\"training_config\"][\"output_dir\"], \"gguf\")\n",
    "    \n",
    "    # Export with q4_k_m quantization (good balance of size and quality)\n",
    "    model.save_pretrained_gguf(\n",
    "        gguf_output_path,\n",
    "        tokenizer,\n",
    "        quantization_method=\"q4_k_m\"  # Options: \"q4_k_m\", \"q8_0\", \"f16\"\n",
    "    )\n",
    "    print(f\"‚úÖ GGUF model exported to {gguf_output_path}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GGUF export requires unsloth. Install it and re-run this cell.\")\n",
    "    print(\"   You can also use llama.cpp to convert the model manually.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6bcf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Ollama Modelfile\n",
    "modelfile_content = '''# Modelfile for Medical LLM\n",
    "# Created from fine-tuned Llama 3 on medical data\n",
    "\n",
    "FROM ./outputs/gguf/model-q4_k_m.gguf\n",
    "\n",
    "# System prompt for medical assistant\n",
    "SYSTEM \"\"\"\n",
    "You are a helpful medical assistant trained to answer medical questions accurately and professionally. \n",
    "\n",
    "Important: Always recommend consulting a qualified healthcare professional for actual medical advice. \n",
    "The information provided is for educational purposes only and should not be used for self-diagnosis or treatment.\n",
    "\"\"\"\n",
    "\n",
    "# Model parameters\n",
    "PARAMETER temperature 0.7\n",
    "PARAMETER top_p 0.9\n",
    "PARAMETER top_k 40\n",
    "PARAMETER repeat_penalty 1.1\n",
    "PARAMETER num_ctx 2048\n",
    "\n",
    "# Template for Llama 3 format\n",
    "TEMPLATE \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{{ .System }}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{{ .Prompt }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "'''\n",
    "\n",
    "modelfile_path = os.path.join(config[\"training_config\"][\"output_dir\"], \"Modelfile\")\n",
    "with open(modelfile_path, 'w') as f:\n",
    "    f.write(modelfile_content)\n",
    "\n",
    "print(f\"‚úÖ Modelfile created at {modelfile_path}\")\n",
    "print(\"\\nüìã To create an Ollama model, run:\")\n",
    "print(f\"   ollama create medical-llama3 -f {modelfile_path}\")\n",
    "print(\"\\nüìã To run the model:\")\n",
    "print(\"   ollama run medical-llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31882eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Push model to HuggingFace Hub\n",
    "# Uncomment to push your model to HuggingFace\n",
    "\n",
    "# hf_repo_name = f\"{config['hugging_face_username']}/{config['model_config']['finetuned_model']}\"\n",
    "# model.push_to_hub(hf_repo_name, tokenizer=tokenizer)\n",
    "# print(f\"‚úÖ Model pushed to HuggingFace: https://huggingface.co/{hf_repo_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4f003c",
   "metadata": {},
   "source": [
    "## 8. Test the Fine-Tuned Model\n",
    "\n",
    "Let's test the fine-tuned model with some medical questions to see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addb9687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable inference mode\n",
    "if UNSLOTH_AVAILABLE:\n",
    "    FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test questions\n",
    "test_questions = [\n",
    "    \"What are the symptoms of diabetes?\",\n",
    "    \"How is hypertension treated?\",\n",
    "    \"What causes pneumonia?\",\n",
    "]\n",
    "\n",
    "# Generate responses\n",
    "for question in test_questions:\n",
    "    # Format the prompt for Llama 3\n",
    "    prompt = f\"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Answer the question truthfully, you are a medical professional.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Tokenize and generate\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=256, \n",
    "            use_cache=True,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # Extract just the answer\n",
    "    if \"<|start_header_id|>assistant<|end_header_id|>\" in response:\n",
    "        answer = response.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1].strip()\n",
    "    else:\n",
    "        answer = response\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚ùì Question: {question}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"ü§ñ Answer: {answer[:500]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9107e846",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You have successfully fine-tuned an LLM on medical data! Here's what you can do next:\n",
    "\n",
    "### Using with Ollama\n",
    "\n",
    "1. Make sure Ollama is installed: https://ollama.ai\n",
    "2. Create the model:\n",
    "   ```bash\n",
    "   ollama create medical-llama3 -f outputs/Modelfile\n",
    "   ```\n",
    "3. Run the model:\n",
    "   ```bash\n",
    "   ollama run medical-llama3\n",
    "   ```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- üìä Train on larger datasets for better performance\n",
    "- üîß Experiment with different LoRA ranks (8, 32, 64)\n",
    "- üìà Increase training epochs\n",
    "- üß™ Evaluate on medical benchmark datasets\n",
    "- üî¨ Try different base models (Mistral, Gemma)\n",
    "\n",
    "### ‚ö†Ô∏è Important Disclaimer\n",
    "\n",
    "This model is for **educational purposes only**. It should NOT be used for:\n",
    "- Medical diagnosis\n",
    "- Treatment recommendations\n",
    "- Clinical decision making\n",
    "\n",
    "Always consult qualified healthcare professionals for medical advice."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
